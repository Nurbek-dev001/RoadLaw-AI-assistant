#!/usr/bin/env python3
"""
–ü–î–î AI Kazakhstan - FastAPI Backend
–°–ø—Ä–∞–≤–æ—á–Ω—ã–π —Å–µ—Ä–≤–∏—Å –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º –¥–æ—Ä–æ–∂–Ω–æ–≥–æ –¥–≤–∏–∂–µ–Ω–∏—è
"""

import os
import sys
import json
import hashlib
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv
from functools import lru_cache

# Load environment variables
load_dotenv()

print("Starting FastAPI backend...")
print(f"Python: {sys.version}")
print(f"Working directory: {os.getcwd()}")

try:
    from fastapi import FastAPI, HTTPException
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel
    import uvicorn
    print("‚úÖ FastAPI imports successful")
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    sys.exit(1)

# Try to import Chroma
try:
    import chromadb
    from chromadb.config import Settings
    CHROMA_AVAILABLE = True
    print("‚úÖ Chroma available")
except ImportError:
    CHROMA_AVAILABLE = False
    print("‚ö†Ô∏è  Chroma not available - using fallback data")

# Try to import OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
    print("‚úÖ OpenAI available")
except ImportError:
    OPENAI_AVAILABLE = False
    print("‚ö†Ô∏è  OpenAI not available - no AI responses")

# Create FastAPI app
app = FastAPI(
    title="üöó –ü–î–î AI Kazakhstan",
    description="RAG-based traffic rules assistant",
    version="0.1.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class Question(BaseModel):
    question: str
    language: str = "ru"

class Answer(BaseModel):
    answer: str
    sources: list
    confidence: float

# Load sample data
def load_sample_data():
    """Load sample –ü–î–î data from JSON"""
    data_path = Path(__file__).parent.parent / "data" / "pdd_sample.json"
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data.get('rules', [])
    except Exception as e:
        print(f"‚ö†Ô∏è  Error loading data: {e}")
        return []

# Load data on startup
PDD_DATA = load_sample_data()
print(f"‚úÖ Loaded {len(PDD_DATA)} –ü–î–î rules")

# Initialize cache for frequently asked questions
answer_cache = {}
cache_hits = 0
cache_misses = 0

def get_cache_key(question: str) -> str:
    """Generate cache key from question"""
    return hashlib.md5(question.lower().strip().encode()).hexdigest()

# Initialize Chroma
client = None
if CHROMA_AVAILABLE:
    try:
        VECTOR_DB_PATH = os.getenv("CHROMA_DB_PATH", "./vector_db/chroma_data")
        client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=VECTOR_DB_PATH,
            anonymized_telemetry=False
        ))
        print(f"‚úÖ Chroma initialized at {VECTOR_DB_PATH}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error initializing Chroma: {e}")
        client = None

# Initialize OpenAI
openai_client = None
if OPENAI_AVAILABLE:
    try:
        api_key = os.getenv("OPENAI_API_KEY", "").strip()
        if api_key and not api_key.startswith("sk-your"):
            openai_client = OpenAI(api_key=api_key)
            print("‚úÖ OpenAI initialized")
        else:
            print("‚ö†Ô∏è  OpenAI API key not configured")
    except Exception as e:
        print(f"‚ö†Ô∏è  Error initializing OpenAI: {e}")

def simple_search(question: str, n_results: int = 3):
    """Simple keyword-based search in –ü–î–î data"""
    # Convert to lowercase for comparison
    q_lower = question.lower()
    
    results = []
    for rule in PDD_DATA:
        # Check if keywords match
        keywords = rule.get('keywords', [])
        title = rule.get('title', '').lower()
        content = rule.get('content', '').lower()
        
        # Calculate relevance
        relevance = 0
        for keyword in keywords:
            if keyword.lower() in q_lower:
                relevance += 0.3
        
        if any(word in q_lower for word in title.split()):
            relevance += 0.5
        
        if any(word in q_lower for word in content.split()):
            relevance += 0.2
        
        if relevance > 0:
            results.append({
                'rule': rule,
                'relevance': min(relevance, 1.0)
            })
    
    # Sort by relevance and return top n
    results = sorted(results, key=lambda x: x['relevance'], reverse=True)
    return results[:n_results]

def chroma_search(question: str, n_results: int = 3):
    """Search using Chroma vector database"""
    if not client:
        return []
    
    try:
        collection = client.get_collection(name="pdd_rules")
        query_results = collection.query(
            query_texts=[question],
            n_results=n_results
        )
        
        results = []
        if query_results['documents'] and len(query_results['documents']) > 0:
            for i, doc in enumerate(query_results['documents'][0]):
                # Find matching rule in PDD_DATA
                for rule in PDD_DATA:
                    if rule['content'] in doc or rule['title'] in doc:
                        distance = query_results['distances'][0][i] if query_results['distances'] else 1.0
                        results.append({
                            'rule': rule,
                            'relevance': 1 - min(distance, 1.0)
                        })
                        break
        
        return results
    except Exception as e:
        print(f"Chroma search error: {e}")
        return []

def search_pdd(question: str, n_results: int = 3):
    """Search –ü–î–î data - try Chroma first, fallback to simple search"""
    # Try Chroma first
    if CHROMA_AVAILABLE:
        results = chroma_search(question, n_results)
        if results:
            return results
    
    # Fallback to simple search
    return simple_search(question, n_results)

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "status": "ok",
        "service": "üöó –ü–î–î AI Kazakhstan",
        "version": "0.1.0",
        "endpoints": [
            "/health",
            "/ask (POST)",
            "/docs"
        ]
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "database": "connected" if client else "fallback",
        "documents": len(PDD_DATA),
        "ai_enabled": openai_client is not None
    }

@app.get("/metrics")
async def get_metrics() -> dict:
    """Get caching and performance metrics"""
    total_queries = cache_hits + cache_misses
    hit_rate = (cache_hits / total_queries * 100) if total_queries > 0 else 0
    
    return {
        "total_queries": total_queries,
        "cache_hits": cache_hits,
        "cache_misses": cache_misses,
        "hit_rate_percent": round(hit_rate, 2),
        "cache_size": len(answer_cache)
    }

@app.post("/ask", response_model=Answer)
async def ask_question(question_data: Question) -> Answer:
    """Main endpoint: Ask a question about –ü–î–î RK"""
    global cache_hits, cache_misses
    
    question = question_data.question.strip()
    
    if not question:
        raise HTTPException(status_code=400, detail="Question cannot be empty")
    
    if len(question) > 500:
        raise HTTPException(status_code=400, detail="Question too long (max 500 chars)")
    
    # Check cache
    cache_key = get_cache_key(question)
    if cache_key in answer_cache:
        cache_hits += 1
        print(f"‚úÖ Cache HIT for: {question[:50]}... (hits: {cache_hits})")
        return answer_cache[cache_key]
    
    cache_misses += 1
    
    try:
        # Search for relevant –ü–î–î info
        search_results = search_pdd(question, n_results=3)
        
        if not search_results:
            result = Answer(
                answer="–í –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –ü–î–î.",
                sources=[],
                confidence=0.0
            )
            answer_cache[cache_key] = result
            return result
        
        # Prepare sources
        sources = []
        context_parts = []
        
        total_relevance = 0
        for result in search_results:
            rule = result['rule']
            relevance = result['relevance']
            
            sources.append({
                'section': rule.get('section', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª'),
                'id': rule.get('id', ''),
                'title': rule.get('title', ''),
                'relevance': relevance
            })
            
            context_parts.append(f"{rule.get('title', '')}: {rule.get('content', '')}")
            total_relevance += relevance
        
        # Generate answer
        answer = None
        if openai_client:
            try:
                system_prompt = """–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ü–î–î –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω.
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–û–±—ä—è—Å–Ω—è–π –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º.
–í—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—É–Ω–∫—Ç.
–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º (1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)."""
                
                response = openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –ü–î–î:\n{chr(10).join(context_parts)}\n\n–í–æ–ø—Ä–æ—Å: {question}"}
                    ],
                    temperature=0.2,
                    max_tokens=300,
                    timeout=10
                )
                answer = response.choices[0].message.content
                print(f"‚úÖ OpenAI response for: {question[:50]}...")
                
            except Exception as e:
                error_msg = str(e)
                if "insufficient_quota" in error_msg:
                    print(f"‚ö†Ô∏è  OpenAI quota exceeded, using fallback")
                else:
                    print(f"‚ö†Ô∏è  OpenAI error: {error_msg}")
                # Fallback to base information
                answer = None
        
        # Fallback answer if OpenAI failed or not available
        if not answer:
            answer = f"–ù–∞–π–¥–µ–Ω–æ –≤ –ü–î–î –†–ö:\n\n{chr(10).join(context_parts)}"
        
        # Calculate confidence
        confidence = min(total_relevance / len(search_results), 1.0) if search_results else 0.0
        
        result = Answer(
            answer=answer,
            sources=sources,
            confidence=confidence
        )
        
        # Cache the result
        answer_cache[cache_key] = result
        
        return result
        
    except Exception as e:
        print(f"Error in /ask: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/docs")
async def docs():
    """API Documentation"""
    return {
        "title": "–ü–î–î AI Kazakhstan API",
        "version": "0.1.0",
        "endpoints": {
            "GET /": "Root endpoint - service info",
            "GET /health": "Health check",
            "POST /ask": "Ask a question about –ü–î–î",
            "GET /docs": "This documentation"
        },
        "example_request": {
            "question": "–ö—Ç–æ —É—Å—Ç—É–ø–∞–µ—Ç –Ω–∞ –∫—Ä—É–≥–µ?",
            "language": "ru"
        },
        "example_response": {
            "answer": "–°–æ–≥–ª–∞—Å–Ω–æ –ü–î–î –†–ö...",
            "sources": [
                {
                    "section": "13. –ü—Ä–æ–µ–∑–¥ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–æ–≤",
                    "id": "13.7",
                    "title": "–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞ –∫—Ä—É–≥–æ–≤–æ–º –¥–≤–∏–∂–µ–Ω–∏–∏",
                    "relevance": 0.95
                }
            ],
            "confidence": 0.92
        }
    }

if __name__ == "__main__":
    print("Starting server on 0.0.0.0:8000...")
    try:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8000,
            log_level="info"
        )
    except Exception as e:
        print(f"‚ùå Server error: {e}")
        sys.exit(1)
